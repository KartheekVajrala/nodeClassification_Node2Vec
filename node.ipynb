{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.4, while the latest version is 1.3.5.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch_geometric.nn import Node2Vec\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embedding(model):\n",
    "    torch.save(model.embedding.weight.data.cpu(), 'embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "walk_length = 20\n",
    "lr = 0.01\n",
    "log_steps = 100\n",
    "walks_per_node = 1\n",
    "context_size = 10\n",
    "embedding_dim = 64\n",
    "\n",
    "dataset = PygNodePropPredDataset(name='ogbn-products')\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Step: 100/19134, Loss: 6.7755\n",
      "Epoch: 01, Step: 200/19134, Loss: 6.6742\n",
      "Epoch: 01, Step: 300/19134, Loss: 6.6339\n",
      "Epoch: 01, Step: 400/19134, Loss: 6.5887\n",
      "Epoch: 01, Step: 500/19134, Loss: 6.4945\n",
      "Epoch: 01, Step: 600/19134, Loss: 6.4333\n",
      "Epoch: 01, Step: 700/19134, Loss: 6.5697\n",
      "Epoch: 01, Step: 800/19134, Loss: 6.6184\n",
      "Epoch: 01, Step: 900/19134, Loss: 6.4451\n",
      "Epoch: 01, Step: 1000/19134, Loss: 6.5900\n",
      "Epoch: 01, Step: 1100/19134, Loss: 6.4612\n",
      "Epoch: 01, Step: 1200/19134, Loss: 6.3273\n",
      "Epoch: 01, Step: 1300/19134, Loss: 6.4619\n",
      "Epoch: 01, Step: 1400/19134, Loss: 6.3759\n",
      "Epoch: 01, Step: 1500/19134, Loss: 6.2002\n",
      "Epoch: 01, Step: 1600/19134, Loss: 6.3404\n",
      "Epoch: 01, Step: 1700/19134, Loss: 6.1633\n",
      "Epoch: 01, Step: 1800/19134, Loss: 6.1413\n",
      "Epoch: 01, Step: 1900/19134, Loss: 6.2510\n",
      "Epoch: 01, Step: 2000/19134, Loss: 5.9049\n",
      "Epoch: 01, Step: 2100/19134, Loss: 6.0133\n",
      "Epoch: 01, Step: 2200/19134, Loss: 6.0696\n",
      "Epoch: 01, Step: 2300/19134, Loss: 6.0149\n",
      "Epoch: 01, Step: 2400/19134, Loss: 6.1405\n",
      "Epoch: 01, Step: 2500/19134, Loss: 5.9674\n",
      "Epoch: 01, Step: 2600/19134, Loss: 5.8691\n",
      "Epoch: 01, Step: 2700/19134, Loss: 5.8656\n",
      "Epoch: 01, Step: 2800/19134, Loss: 5.7471\n",
      "Epoch: 01, Step: 2900/19134, Loss: 5.7000\n",
      "Epoch: 01, Step: 3000/19134, Loss: 5.8918\n",
      "Epoch: 01, Step: 3100/19134, Loss: 5.8736\n",
      "Epoch: 01, Step: 3200/19134, Loss: 5.7221\n",
      "Epoch: 01, Step: 3300/19134, Loss: 5.6129\n",
      "Epoch: 01, Step: 3400/19134, Loss: 5.7350\n",
      "Epoch: 01, Step: 3500/19134, Loss: 5.5640\n",
      "Epoch: 01, Step: 3600/19134, Loss: 5.5980\n",
      "Epoch: 01, Step: 3700/19134, Loss: 5.6511\n",
      "Epoch: 01, Step: 3800/19134, Loss: 5.5354\n",
      "Epoch: 01, Step: 3900/19134, Loss: 5.5461\n",
      "Epoch: 01, Step: 4000/19134, Loss: 5.4008\n",
      "Epoch: 01, Step: 4100/19134, Loss: 5.3002\n",
      "Epoch: 01, Step: 4200/19134, Loss: 5.3991\n",
      "Epoch: 01, Step: 4300/19134, Loss: 5.3644\n",
      "Epoch: 01, Step: 4400/19134, Loss: 5.2776\n",
      "Epoch: 01, Step: 4500/19134, Loss: 5.1493\n",
      "Epoch: 01, Step: 4600/19134, Loss: 5.1772\n",
      "Epoch: 01, Step: 4700/19134, Loss: 5.1730\n",
      "Epoch: 01, Step: 4800/19134, Loss: 5.1702\n",
      "Epoch: 01, Step: 4900/19134, Loss: 5.1666\n",
      "Epoch: 01, Step: 5000/19134, Loss: 5.0225\n",
      "Epoch: 01, Step: 5100/19134, Loss: 5.0452\n",
      "Epoch: 01, Step: 5200/19134, Loss: 5.1680\n",
      "Epoch: 01, Step: 5300/19134, Loss: 5.1193\n",
      "Epoch: 01, Step: 5400/19134, Loss: 5.0302\n",
      "Epoch: 01, Step: 5500/19134, Loss: 5.0090\n",
      "Epoch: 01, Step: 5600/19134, Loss: 4.8996\n",
      "Epoch: 01, Step: 5700/19134, Loss: 5.0309\n",
      "Epoch: 01, Step: 5800/19134, Loss: 4.8625\n",
      "Epoch: 01, Step: 5900/19134, Loss: 4.8071\n",
      "Epoch: 01, Step: 6000/19134, Loss: 4.8807\n",
      "Epoch: 01, Step: 6100/19134, Loss: 4.7010\n",
      "Epoch: 01, Step: 6200/19134, Loss: 4.8544\n",
      "Epoch: 01, Step: 6300/19134, Loss: 4.7110\n",
      "Epoch: 01, Step: 6400/19134, Loss: 4.7181\n",
      "Epoch: 01, Step: 6500/19134, Loss: 4.7867\n",
      "Epoch: 01, Step: 6600/19134, Loss: 4.7621\n",
      "Epoch: 01, Step: 6700/19134, Loss: 4.7058\n",
      "Epoch: 01, Step: 6800/19134, Loss: 4.6247\n",
      "Epoch: 01, Step: 6900/19134, Loss: 4.6618\n",
      "Epoch: 01, Step: 7000/19134, Loss: 4.4866\n",
      "Epoch: 01, Step: 7100/19134, Loss: 4.4847\n",
      "Epoch: 01, Step: 7200/19134, Loss: 4.5023\n",
      "Epoch: 01, Step: 7300/19134, Loss: 4.3610\n",
      "Epoch: 01, Step: 7400/19134, Loss: 4.5251\n",
      "Epoch: 01, Step: 7500/19134, Loss: 4.4497\n",
      "Epoch: 01, Step: 7600/19134, Loss: 4.4450\n",
      "Epoch: 01, Step: 7700/19134, Loss: 4.4510\n",
      "Epoch: 01, Step: 7800/19134, Loss: 4.2548\n",
      "Epoch: 01, Step: 7900/19134, Loss: 4.3842\n",
      "Epoch: 01, Step: 8000/19134, Loss: 4.2911\n",
      "Epoch: 01, Step: 8100/19134, Loss: 4.2447\n",
      "Epoch: 01, Step: 8200/19134, Loss: 4.3190\n",
      "Epoch: 01, Step: 8300/19134, Loss: 4.3482\n",
      "Epoch: 01, Step: 8400/19134, Loss: 4.3549\n",
      "Epoch: 01, Step: 8500/19134, Loss: 4.0975\n",
      "Epoch: 01, Step: 8600/19134, Loss: 4.3680\n",
      "Epoch: 01, Step: 8700/19134, Loss: 4.2396\n",
      "Epoch: 01, Step: 8800/19134, Loss: 4.1880\n",
      "Epoch: 01, Step: 8900/19134, Loss: 4.1537\n",
      "Epoch: 01, Step: 9000/19134, Loss: 4.1959\n",
      "Epoch: 01, Step: 9100/19134, Loss: 4.0404\n",
      "Epoch: 01, Step: 9200/19134, Loss: 4.0456\n",
      "Epoch: 01, Step: 9300/19134, Loss: 4.1141\n",
      "Epoch: 01, Step: 9400/19134, Loss: 3.9280\n",
      "Epoch: 01, Step: 9500/19134, Loss: 4.0428\n",
      "Epoch: 01, Step: 9600/19134, Loss: 3.9676\n",
      "Epoch: 01, Step: 9700/19134, Loss: 4.0986\n",
      "Epoch: 01, Step: 9800/19134, Loss: 3.9719\n",
      "Epoch: 01, Step: 9900/19134, Loss: 3.9726\n",
      "Epoch: 01, Step: 10000/19134, Loss: 3.8195\n",
      "Epoch: 01, Step: 10100/19134, Loss: 3.8575\n",
      "Epoch: 01, Step: 10200/19134, Loss: 3.8722\n",
      "Epoch: 01, Step: 10300/19134, Loss: 3.8145\n",
      "Epoch: 01, Step: 10400/19134, Loss: 3.9093\n",
      "Epoch: 01, Step: 10500/19134, Loss: 3.7906\n",
      "Epoch: 01, Step: 10600/19134, Loss: 3.8441\n",
      "Epoch: 01, Step: 10700/19134, Loss: 3.8493\n",
      "Epoch: 01, Step: 10800/19134, Loss: 3.7199\n",
      "Epoch: 01, Step: 10900/19134, Loss: 3.7589\n",
      "Epoch: 01, Step: 11000/19134, Loss: 3.6863\n",
      "Epoch: 01, Step: 11100/19134, Loss: 3.7789\n",
      "Epoch: 01, Step: 11200/19134, Loss: 3.6991\n",
      "Epoch: 01, Step: 11300/19134, Loss: 3.7433\n",
      "Epoch: 01, Step: 11400/19134, Loss: 3.7061\n",
      "Epoch: 01, Step: 11500/19134, Loss: 3.6394\n",
      "Epoch: 01, Step: 11600/19134, Loss: 3.6049\n",
      "Epoch: 01, Step: 11700/19134, Loss: 3.6141\n",
      "Epoch: 01, Step: 11800/19134, Loss: 3.5985\n",
      "Epoch: 01, Step: 11900/19134, Loss: 3.5602\n",
      "Epoch: 01, Step: 12000/19134, Loss: 3.5260\n",
      "Epoch: 01, Step: 12100/19134, Loss: 3.6169\n",
      "Epoch: 01, Step: 12200/19134, Loss: 3.4493\n",
      "Epoch: 01, Step: 12300/19134, Loss: 3.5688\n",
      "Epoch: 01, Step: 12400/19134, Loss: 3.5901\n",
      "Epoch: 01, Step: 12500/19134, Loss: 3.4973\n",
      "Epoch: 01, Step: 12600/19134, Loss: 3.4375\n",
      "Epoch: 01, Step: 12700/19134, Loss: 3.4711\n",
      "Epoch: 01, Step: 12800/19134, Loss: 3.3808\n",
      "Epoch: 01, Step: 12900/19134, Loss: 3.4679\n",
      "Epoch: 01, Step: 13000/19134, Loss: 3.3827\n",
      "Epoch: 01, Step: 13100/19134, Loss: 3.4086\n",
      "Epoch: 01, Step: 13200/19134, Loss: 3.3737\n",
      "Epoch: 01, Step: 13300/19134, Loss: 3.4386\n",
      "Epoch: 01, Step: 13400/19134, Loss: 3.3089\n",
      "Epoch: 01, Step: 13500/19134, Loss: 3.3018\n",
      "Epoch: 01, Step: 13600/19134, Loss: 3.2863\n",
      "Epoch: 01, Step: 13700/19134, Loss: 3.3230\n",
      "Epoch: 01, Step: 13800/19134, Loss: 3.3166\n",
      "Epoch: 01, Step: 13900/19134, Loss: 3.2813\n",
      "Epoch: 01, Step: 14000/19134, Loss: 3.3203\n",
      "Epoch: 01, Step: 14100/19134, Loss: 3.2484\n",
      "Epoch: 01, Step: 14200/19134, Loss: 3.3172\n",
      "Epoch: 01, Step: 14300/19134, Loss: 3.1762\n",
      "Epoch: 01, Step: 14400/19134, Loss: 3.1870\n",
      "Epoch: 01, Step: 14500/19134, Loss: 3.2842\n",
      "Epoch: 01, Step: 14600/19134, Loss: 3.1487\n",
      "Epoch: 01, Step: 14700/19134, Loss: 3.1808\n",
      "Epoch: 01, Step: 14800/19134, Loss: 3.0560\n",
      "Epoch: 01, Step: 14900/19134, Loss: 3.0784\n",
      "Epoch: 01, Step: 15000/19134, Loss: 3.1411\n",
      "Epoch: 01, Step: 15100/19134, Loss: 3.1371\n",
      "Epoch: 01, Step: 15200/19134, Loss: 3.0509\n",
      "Epoch: 01, Step: 15300/19134, Loss: 3.1193\n",
      "Epoch: 01, Step: 15400/19134, Loss: 3.1197\n",
      "Epoch: 01, Step: 15500/19134, Loss: 3.0594\n",
      "Epoch: 01, Step: 15600/19134, Loss: 3.0544\n",
      "Epoch: 01, Step: 15700/19134, Loss: 3.0525\n",
      "Epoch: 01, Step: 15800/19134, Loss: 2.9926\n",
      "Epoch: 01, Step: 15900/19134, Loss: 3.0339\n",
      "Epoch: 01, Step: 16000/19134, Loss: 2.9395\n",
      "Epoch: 01, Step: 16100/19134, Loss: 2.8482\n",
      "Epoch: 01, Step: 16200/19134, Loss: 2.8652\n",
      "Epoch: 01, Step: 16300/19134, Loss: 2.9851\n",
      "Epoch: 01, Step: 16400/19134, Loss: 3.0019\n",
      "Epoch: 01, Step: 16500/19134, Loss: 2.9621\n",
      "Epoch: 01, Step: 16600/19134, Loss: 2.9341\n",
      "Epoch: 01, Step: 16700/19134, Loss: 2.9824\n",
      "Epoch: 01, Step: 16800/19134, Loss: 2.8726\n",
      "Epoch: 01, Step: 16900/19134, Loss: 2.8990\n",
      "Epoch: 01, Step: 17000/19134, Loss: 2.8878\n",
      "Epoch: 01, Step: 17100/19134, Loss: 2.9332\n",
      "Epoch: 01, Step: 17200/19134, Loss: 2.8989\n",
      "Epoch: 01, Step: 17300/19134, Loss: 2.8293\n",
      "Epoch: 01, Step: 17400/19134, Loss: 2.7749\n",
      "Epoch: 01, Step: 17500/19134, Loss: 2.7882\n",
      "Epoch: 01, Step: 17600/19134, Loss: 2.8548\n",
      "Epoch: 01, Step: 17700/19134, Loss: 2.7145\n",
      "Epoch: 01, Step: 17800/19134, Loss: 2.7103\n",
      "Epoch: 01, Step: 17900/19134, Loss: 2.7352\n",
      "Epoch: 01, Step: 18000/19134, Loss: 2.7548\n",
      "Epoch: 01, Step: 18100/19134, Loss: 2.8096\n",
      "Epoch: 01, Step: 18200/19134, Loss: 2.7044\n",
      "Epoch: 01, Step: 18300/19134, Loss: 2.7641\n",
      "Epoch: 01, Step: 18400/19134, Loss: 2.6821\n",
      "Epoch: 01, Step: 18500/19134, Loss: 2.6773\n",
      "Epoch: 01, Step: 18600/19134, Loss: 2.6920\n",
      "Epoch: 01, Step: 18700/19134, Loss: 2.6904\n",
      "Epoch: 01, Step: 18800/19134, Loss: 2.6622\n",
      "Epoch: 01, Step: 18900/19134, Loss: 2.6324\n",
      "Epoch: 01, Step: 19000/19134, Loss: 2.6646\n",
      "Epoch: 01, Step: 19100/19134, Loss: 2.6237\n"
     ]
    }
   ],
   "source": [
    "model = Node2Vec(data.edge_index,embedding_dim, walk_length,\n",
    "                    context_size, walks_per_node,\n",
    "                    sparse=True).to(device)\n",
    "\n",
    "loader = model.loader(batch_size=batch_size, shuffle=True,\n",
    "                        num_workers=0)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=lr)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for i, (pos_rw, neg_rw) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % log_steps == 0:\n",
    "            print(f'Epoch: {epoch:02d}, Step: {i+1:03d}/{len(loader)}, 'f'Loss: {loss:.4f}')\n",
    "    save_embedding(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "z = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_mask = train_mask = torch.zeros(data.num_nodes,dtype=torch.long, device=device)\n",
    "x_mask[0] = 1\n",
    "node_embeddings = z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = node_embeddings.cpu().detach().numpy()\n",
    "y = data.y.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,train_size=0.8,test_size=0.2,random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree_model = DecisionTreeClassifier(max_depth = 8).fit(X_train, y_train)\n",
    "dtree_predictions = dtree_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, dtree_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7732183762550888\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, dtree_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('GML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e227493907f45da4be8dd92e416451ade0b7afc91e93ef6983e4451a2178ec1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
